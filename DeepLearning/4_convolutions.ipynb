{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.617807\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.4%\n",
      "Minibatch loss at step 50: 1.763763\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 56.5%\n",
      "Minibatch loss at step 100: 0.749195\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 150: 0.980316\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 200: 0.234219\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 250: 0.538514\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 300: 0.780967\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 350: 0.463289\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 400: 1.256397\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 450: 0.933962\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 500: 0.523672\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 550: 0.312279\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 600: 0.257630\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 650: 0.845425\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 700: 0.464060\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 750: 0.900681\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 800: 0.500590\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 850: 0.873447\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 900: 0.640185\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 950: 1.096303\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1000: 0.374095\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 89.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# print(28*28)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size, image_size, 1))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_training = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "    def getLogits(inputs):\n",
    "        with tf.variable_scope('conv1'):\n",
    "            conv1 = tf.layers.conv2d(inputs=inputs, filters=32, kernel_size=(1,1), padding='same')\n",
    "            conv1 = tf.layers.conv2d(inputs=conv1, filters=32, kernel_size=(1,3), padding='same')\n",
    "            conv1 = tf.layers.conv2d(inputs=conv1, filters=32, kernel_size=(3,1), padding='same')\n",
    "            conv1 = tf.layers.batch_normalization(conv1)\n",
    "            conv1 = tf.nn.relu(conv1)\n",
    "            conv1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "            conv1 = tf.layers.dropout(inputs=conv1,rate=0.25,noise_shape=None,seed=None,training=tf_training)\n",
    "        \n",
    "        with tf.variable_scope('conv2'):\n",
    "            conv2 = tf.layers.conv2d(inputs=conv1, filters=64, kernel_size=(1,1), padding='same')\n",
    "            conv2 = tf.layers.conv2d(inputs=conv2, filters=64, kernel_size=(1,3), padding='same')\n",
    "            conv2 = tf.layers.conv2d(inputs=conv2, filters=64, kernel_size=(3,1), padding='same')\n",
    "            conv2 = tf.layers.batch_normalization(conv2)\n",
    "            conv2 = tf.nn.relu(conv2)\n",
    "            conv2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "            conv2 = tf.layers.dropout(inputs=conv2,rate=0.25,noise_shape=None,seed=None,training=tf_training)\n",
    "\n",
    "        with tf.variable_scope('conv3'):\n",
    "            conv3 = tf.layers.conv2d(inputs=conv2, filters=128, kernel_size=(1,1), padding='same')\n",
    "            conv3 = tf.layers.conv2d(inputs=conv3, filters=128, kernel_size=(1,3), padding='same')\n",
    "            conv3 = tf.layers.conv2d(inputs=conv3, filters=128, kernel_size=(3,1), padding='same')\n",
    "            conv3 = tf.layers.batch_normalization(conv3)\n",
    "            conv3 = tf.nn.relu(conv3)\n",
    "            conv3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "            conv3 = tf.layers.dropout(inputs=conv3,rate=0.25,noise_shape=None,seed=None,training=tf_training)\n",
    "\n",
    "            \n",
    "        with tf.variable_scope('flatten'):\n",
    "            flatten = tf.contrib.layers.flatten(conv3)\n",
    "        \n",
    "        with tf.variable_scope('dense1'):\n",
    "            hidden = tf.layers.dense(inputs=flatten, units=1024)\n",
    "            hidden = tf.layers.batch_normalization(hidden)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "            hidden = tf.layers.dropout(inputs=hidden,rate=0.25,noise_shape=None,seed=None,training=tf_training)\n",
    "\n",
    "        with tf.variable_scope('dense2'):\n",
    "            hidden = tf.layers.dense(inputs=hidden, units=256)\n",
    "            hidden = tf.layers.batch_normalization(hidden)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "            hidden = tf.layers.dropout(inputs=hidden,rate=0.25,noise_shape=None,seed=None,training=tf_training)\n",
    "        \n",
    "        with tf.variable_scope('dense3'):\n",
    "            hidden = tf.layers.dense(inputs=hidden, units=64)\n",
    "            hidden = tf.layers.batch_normalization(hidden)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "            hidden = tf.layers.dropout(inputs=hidden,rate=0.25,noise_shape=None,seed=None,training=tf_training)\n",
    "        \n",
    "        with tf.variable_scope('dense4'):\n",
    "            logits = tf.layers.dense(inputs=hidden, units=10, activation=tf.nn.relu)\n",
    "            \n",
    "        return logits\n",
    "    \n",
    "    logits = getLogits(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 100, 0.99, staircase=True)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# merged = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter('./tensorboardlog', graph)\n",
    "# summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Learning rate: 0.100000, Global Step: 1\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 9.9%\n",
      "Learning rate: 0.099000, Global Step: 101\n",
      "Minibatch accuracy: 49.6%\n",
      "Validation accuracy: 65.5%\n",
      "Learning rate: 0.098010, Global Step: 201\n",
      "Minibatch accuracy: 74.6%\n",
      "Validation accuracy: 80.1%\n",
      "Learning rate: 0.097030, Global Step: 301\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 83.9%\n",
      "Learning rate: 0.096060, Global Step: 401\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.6%\n",
      "Learning rate: 0.095099, Global Step: 501\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 85.7%\n",
      "Learning rate: 0.094148, Global Step: 601\n",
      "Minibatch accuracy: 83.2%\n",
      "Validation accuracy: 86.0%\n",
      "Learning rate: 0.093207, Global Step: 701\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 86.5%\n",
      "Learning rate: 0.092274, Global Step: 801\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 87.1%\n",
      "Learning rate: 0.091352, Global Step: 901\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 86.8%\n",
      "Learning rate: 0.090438, Global Step: 1001\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 87.5%\n",
      "Learning rate: 0.089534, Global Step: 1101\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.9%\n",
      "Learning rate: 0.088638, Global Step: 1201\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.7%\n",
      "Learning rate: 0.087752, Global Step: 1301\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.1%\n",
      "Learning rate: 0.086875, Global Step: 1401\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.3%\n",
      "Learning rate: 0.086006, Global Step: 1501\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "Learning rate: 0.085146, Global Step: 1601\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.8%\n",
      "Learning rate: 0.084294, Global Step: 1701\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.9%\n",
      "Learning rate: 0.083451, Global Step: 1801\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "Learning rate: 0.082617, Global Step: 1901\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 89.1%\n",
      "Learning rate: 0.081791, Global Step: 2001\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 89.2%\n",
      "Learning rate: 0.080973, Global Step: 2101\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.2%\n",
      "Learning rate: 0.080163, Global Step: 2201\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.3%\n",
      "Learning rate: 0.079361, Global Step: 2301\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 89.5%\n",
      "Learning rate: 0.078568, Global Step: 2401\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.6%\n",
      "Learning rate: 0.077782, Global Step: 2501\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 89.6%\n",
      "Learning rate: 0.077004, Global Step: 2601\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.7%\n",
      "Learning rate: 0.076234, Global Step: 2701\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 89.8%\n",
      "Learning rate: 0.075472, Global Step: 2801\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 89.9%\n",
      "Learning rate: 0.074717, Global Step: 2901\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 89.9%\n",
      "Learning rate: 0.073970, Global Step: 3001\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 90.0%\n",
      "Learning rate: 0.073230, Global Step: 3101\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 90.0%\n",
      "Learning rate: 0.072498, Global Step: 3201\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Learning rate: 0.071773, Global Step: 3301\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 90.1%\n",
      "Learning rate: 0.071055, Global Step: 3401\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 90.2%\n",
      "Learning rate: 0.070345, Global Step: 3501\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Learning rate: 0.069641, Global Step: 3601\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.2%\n",
      "Learning rate: 0.068945, Global Step: 3701\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 90.2%\n",
      "Learning rate: 0.068255, Global Step: 3801\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.4%\n",
      "Learning rate: 0.067573, Global Step: 3901\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.4%\n",
      "Learning rate: 0.066897, Global Step: 4001\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 90.4%\n",
      "Learning rate: 0.066228, Global Step: 4101\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.4%\n",
      "Learning rate: 0.065566, Global Step: 4201\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 90.6%\n",
      "Learning rate: 0.064910, Global Step: 4301\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.5%\n",
      "Learning rate: 0.064261, Global Step: 4401\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 90.5%\n",
      "Learning rate: 0.063619, Global Step: 4501\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 90.4%\n",
      "Learning rate: 0.062982, Global Step: 4601\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 90.5%\n",
      "Learning rate: 0.062353, Global Step: 4701\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 90.8%\n",
      "Learning rate: 0.061729, Global Step: 4801\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 90.8%\n",
      "Learning rate: 0.061112, Global Step: 4901\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 90.6%\n",
      "Learning rate: 0.060501, Global Step: 5001\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 90.7%\n",
      "Learning rate: 0.059896, Global Step: 5101\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.6%\n",
      "Learning rate: 0.059297, Global Step: 5201\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 90.8%\n",
      "Learning rate: 0.058704, Global Step: 5301\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.8%\n",
      "Learning rate: 0.058117, Global Step: 5401\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.7%\n",
      "Learning rate: 0.057536, Global Step: 5501\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.7%\n",
      "Learning rate: 0.056960, Global Step: 5601\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.9%\n",
      "Learning rate: 0.056391, Global Step: 5701\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.0%\n",
      "Learning rate: 0.055827, Global Step: 5801\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 90.9%\n",
      "Learning rate: 0.055268, Global Step: 5901\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.0%\n",
      "Learning rate: 0.054716, Global Step: 6001\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Learning rate: 0.054169, Global Step: 6101\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 91.1%\n",
      "Learning rate: 0.053627, Global Step: 6201\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 91.0%\n",
      "Learning rate: 0.053091, Global Step: 6301\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.0%\n",
      "Learning rate: 0.052560, Global Step: 6401\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 91.0%\n",
      "Learning rate: 0.052034, Global Step: 6501\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.9%\n",
      "Learning rate: 0.051514, Global Step: 6601\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.1%\n",
      "Learning rate: 0.050999, Global Step: 6701\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 91.2%\n",
      "Learning rate: 0.050489, Global Step: 6801\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Learning rate: 0.049984, Global Step: 6901\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 91.1%\n",
      "Learning rate: 0.049484, Global Step: 7001\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 91.3%\n",
      "Learning rate: 0.048989, Global Step: 7101\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Learning rate: 0.048499, Global Step: 7201\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 91.2%\n",
      "Learning rate: 0.048014, Global Step: 7301\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Learning rate: 0.047534, Global Step: 7401\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 91.3%\n",
      "Learning rate: 0.047059, Global Step: 7501\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 91.3%\n",
      "Learning rate: 0.046588, Global Step: 7601\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Learning rate: 0.046122, Global Step: 7701\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.2%\n",
      "Learning rate: 0.045661, Global Step: 7801\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.045204, Global Step: 7901\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.044752, Global Step: 8001\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 91.3%\n",
      "Learning rate: 0.044305, Global Step: 8101\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.4%\n",
      "Learning rate: 0.043862, Global Step: 8201\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 91.4%\n",
      "Learning rate: 0.043423, Global Step: 8301\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.042989, Global Step: 8401\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 91.4%\n",
      "Learning rate: 0.042559, Global Step: 8501\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 91.4%\n",
      "Learning rate: 0.042133, Global Step: 8601\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.3%\n",
      "Learning rate: 0.041712, Global Step: 8701\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.041295, Global Step: 8801\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.040882, Global Step: 8901\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.040473, Global Step: 9001\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.040068, Global Step: 9101\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.039668, Global Step: 9201\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.039271, Global Step: 9301\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.038878, Global Step: 9401\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.038490, Global Step: 9501\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.038105, Global Step: 9601\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.037724, Global Step: 9701\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.037346, Global Step: 9801\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.036973, Global Step: 9901\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.036603, Global Step: 10001\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.036237, Global Step: 10101\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.035875, Global Step: 10201\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.035516, Global Step: 10301\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.035161, Global Step: 10401\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.034809, Global Step: 10501\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.034461, Global Step: 10601\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.034117, Global Step: 10701\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.033775, Global Step: 10801\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.033438, Global Step: 10901\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.033103, Global Step: 11001\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.032772, Global Step: 11101\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.032445, Global Step: 11201\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Learning rate: 0.032120, Global Step: 11301\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.031799, Global Step: 11401\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.031481, Global Step: 11501\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.031166, Global Step: 11601\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Learning rate: 0.030854, Global Step: 11701\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.030546, Global Step: 11801\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.030240, Global Step: 11901\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.029938, Global Step: 12001\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.029639, Global Step: 12101\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.029342, Global Step: 12201\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.029049, Global Step: 12301\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.028758, Global Step: 12401\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.028471, Global Step: 12501\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.028186, Global Step: 12601\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.027904, Global Step: 12701\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.027625, Global Step: 12801\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.027349, Global Step: 12901\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.027075, Global Step: 13001\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.026805, Global Step: 13101\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.026537, Global Step: 13201\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.026271, Global Step: 13301\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.026009, Global Step: 13401\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.025748, Global Step: 13501\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.025491, Global Step: 13601\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.7%\n",
      "Learning rate: 0.025236, Global Step: 13701\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.024984, Global Step: 13801\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.024734, Global Step: 13901\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.024487, Global Step: 14001\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.024242, Global Step: 14101\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.023999, Global Step: 14201\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.023759, Global Step: 14301\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.023522, Global Step: 14401\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.023286, Global Step: 14501\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.023054, Global Step: 14601\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.022823, Global Step: 14701\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.022595, Global Step: 14801\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.022369, Global Step: 14901\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.022145, Global Step: 15001\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.021924, Global Step: 15101\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 91.8%\n",
      "Learning rate: 0.021705, Global Step: 15201\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.021487, Global Step: 15301\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.021273, Global Step: 15401\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.021060, Global Step: 15501\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.020849, Global Step: 15601\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.020641, Global Step: 15701\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.020434, Global Step: 15801\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.020230, Global Step: 15901\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.020028, Global Step: 16001\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.019827, Global Step: 16101\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.019629, Global Step: 16201\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.019433, Global Step: 16301\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.019239, Global Step: 16401\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.019046, Global Step: 16501\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.018856, Global Step: 16601\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.018667, Global Step: 16701\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.018480, Global Step: 16801\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.018296, Global Step: 16901\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.018113, Global Step: 17001\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.017932, Global Step: 17101\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.017752, Global Step: 17201\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.017575, Global Step: 17301\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.017399, Global Step: 17401\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.017225, Global Step: 17501\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.017053, Global Step: 17601\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.016882, Global Step: 17701\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.016713, Global Step: 17801\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.016546, Global Step: 17901\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.016381, Global Step: 18001\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.016217, Global Step: 18101\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.016055, Global Step: 18201\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.015894, Global Step: 18301\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.015735, Global Step: 18401\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.015578, Global Step: 18501\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.015422, Global Step: 18601\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.015268, Global Step: 18701\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.015115, Global Step: 18801\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.014964, Global Step: 18901\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.014815, Global Step: 19001\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.014666, Global Step: 19101\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 91.9%\n",
      "Learning rate: 0.014520, Global Step: 19201\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.014375, Global Step: 19301\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.014231, Global Step: 19401\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.014088, Global Step: 19501\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.013948, Global Step: 19601\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.013808, Global Step: 19701\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.013670, Global Step: 19801\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.013533, Global Step: 19901\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.013398, Global Step: 20001\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.013264, Global Step: 20101\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.013131, Global Step: 20201\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.013000, Global Step: 20301\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.012870, Global Step: 20401\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.012741, Global Step: 20501\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.012614, Global Step: 20601\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.012488, Global Step: 20701\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.012363, Global Step: 20801\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.012239, Global Step: 20901\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.012117, Global Step: 21001\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.011996, Global Step: 21101\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.011876, Global Step: 21201\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.011757, Global Step: 21301\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.011639, Global Step: 21401\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.011523, Global Step: 21501\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.011408, Global Step: 21601\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.011294, Global Step: 21701\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.011181, Global Step: 21801\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.011069, Global Step: 21901\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.010958, Global Step: 22001\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.010849, Global Step: 22101\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.010740, Global Step: 22201\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.010633, Global Step: 22301\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.010527, Global Step: 22401\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.010421, Global Step: 22501\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.010317, Global Step: 22601\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.010214, Global Step: 22701\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.010112, Global Step: 22801\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.010011, Global Step: 22901\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.0%\n",
      "Learning rate: 0.009911, Global Step: 23001\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.009811, Global Step: 23101\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.009713, Global Step: 23201\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.009616, Global Step: 23301\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.009520, Global Step: 23401\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.009425, Global Step: 23501\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.009331, Global Step: 23601\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.009237, Global Step: 23701\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.009145, Global Step: 23801\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.009053, Global Step: 23901\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.008963, Global Step: 24001\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.008873, Global Step: 24101\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.008785, Global Step: 24201\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.008697, Global Step: 24301\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.008610, Global Step: 24401\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.008524, Global Step: 24501\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.008438, Global Step: 24601\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.008354, Global Step: 24701\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.008270, Global Step: 24801\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.008188, Global Step: 24901\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.008106, Global Step: 25001\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.008025, Global Step: 25101\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007945, Global Step: 25201\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007865, Global Step: 25301\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007786, Global Step: 25401\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.007709, Global Step: 25501\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007632, Global Step: 25601\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007555, Global Step: 25701\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007480, Global Step: 25801\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007405, Global Step: 25901\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007331, Global Step: 26001\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007257, Global Step: 26101\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007185, Global Step: 26201\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007113, Global Step: 26301\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.007042, Global Step: 26401\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006972, Global Step: 26501\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006902, Global Step: 26601\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006833, Global Step: 26701\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006764, Global Step: 26801\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006697, Global Step: 26901\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006630, Global Step: 27001\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006564, Global Step: 27101\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006498, Global Step: 27201\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.006433, Global Step: 27301\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.006369, Global Step: 27401\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.006305, Global Step: 27501\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.1%\n",
      "Learning rate: 0.006242, Global Step: 27601\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006179, Global Step: 27701\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006118, Global Step: 27801\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.006056, Global Step: 27901\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005996, Global Step: 28001\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005936, Global Step: 28101\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005877, Global Step: 28201\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005818, Global Step: 28301\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005760, Global Step: 28401\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005702, Global Step: 28501\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005645, Global Step: 28601\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005589, Global Step: 28701\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 92.3%\n",
      "Learning rate: 0.005533, Global Step: 28801\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005477, Global Step: 28901\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005423, Global Step: 29001\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005368, Global Step: 29101\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 92.3%\n",
      "Learning rate: 0.005315, Global Step: 29201\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005262, Global Step: 29301\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Learning rate: 0.005209, Global Step: 29401\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.3%\n",
      "Learning rate: 0.005157, Global Step: 29501\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005105, Global Step: 29601\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005054, Global Step: 29701\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.2%\n",
      "Learning rate: 0.005004, Global Step: 29801\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 92.3%\n",
      "Learning rate: 0.004954, Global Step: 29901\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Learning rate: 0.004904, Global Step: 30001\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.2%\n",
      "Test accuracy: 97.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    merged = tf.summary.merge_all()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_training: True}\n",
    "        \n",
    "        _, summary, predictions = session.run([optimizer, merged, train_prediction], feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary, step)\n",
    "        if (step % 100 == 0):\n",
    "#             print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Learning rate: %f, Global Step: %d\" % (learning_rate.eval(), global_step.eval()))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            feed_dict = {tf_train_dataset : valid_dataset, tf_train_labels : valid_labels, tf_training: False}\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(session.run(valid_prediction, feed_dict=feed_dict), valid_labels))\n",
    "    feed_dict = {tf_train_dataset : test_dataset, tf_train_labels : test_labels, tf_training: False} \n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(session.run(test_prediction, feed_dict=feed_dict), test_labels))\n",
    "    \n",
    "summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
