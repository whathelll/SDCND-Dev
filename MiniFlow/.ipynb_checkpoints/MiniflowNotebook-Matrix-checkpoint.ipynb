{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the linear version with matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for layers in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_layers`: A list of layers with edges into this layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_layers=[]):\n",
    "        \"\"\"\n",
    "        Layer's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all layers need.\n",
    "        \"\"\"\n",
    "        # A list of layers with edges into this layer.\n",
    "        self.inbound_layers = inbound_layers\n",
    "        # The eventual value of this layer. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of layers that this layer outputs to.\n",
    "        self.outbound_layers = []\n",
    "        # Sets this layer as an outbound layer for all of\n",
    "        # this layer's inputs.\n",
    "        for layer in inbound_layers:\n",
    "            layer.outbound_layers.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every layer that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Layer.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    Represents a layer that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Layer) constructor. Weights and bias\n",
    "        # are treated like inbound layers.\n",
    "        Layer.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_layers[0].value\n",
    "        W = self.inbound_layers[1].value\n",
    "        b = self.inbound_layers[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    Represents a layer that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer):\n",
    "        # The base class constructor.\n",
    "        Layer.__init__(self, [layer])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_layers[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "\n",
    "class MSE(Layer):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last layer for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Layer.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_layers[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_layers[1].value.reshape(-1, 1)\n",
    "        # TODO: your code here\n",
    "#         print(y)\n",
    "#         print(a)\n",
    "        self.value = np.power((y - a), 2).mean()\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the layers in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Layer and the value is the respective value feed to that Layer.\n",
    "\n",
    "    Returns a list of sorted layers.\n",
    "    \"\"\"\n",
    "\n",
    "    input_layers = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    layers = [n for n in input_layers]\n",
    "    while len(layers) > 0:\n",
    "        n = layers.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_layers:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            layers.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_layers)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_layers:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Layers.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test your MSE method with this script!\n",
    "\n",
    "No changes necessary, but feel free to play\n",
    "with this script to test your network.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "# from miniflow import *\n",
    "\n",
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a) \n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient descent.\n",
    "    \n",
    "    # Return the new value for x\n",
    "    return x - learning_rate * gradx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: Cost = 86.000, x = 18.000\n",
      "EPOCH 1: Cost = 85.968, x = 17.996\n",
      "EPOCH 2: Cost = 85.935, x = 17.993\n",
      "EPOCH 3: Cost = 85.903, x = 17.989\n",
      "EPOCH 4: Cost = 85.870, x = 17.986\n",
      "EPOCH 5: Cost = 85.838, x = 17.982\n",
      "EPOCH 6: Cost = 85.806, x = 17.978\n",
      "EPOCH 7: Cost = 85.773, x = 17.975\n",
      "EPOCH 8: Cost = 85.741, x = 17.971\n",
      "EPOCH 9: Cost = 85.709, x = 17.968\n",
      "EPOCH 10: Cost = 85.677, x = 17.964\n",
      "EPOCH 11: Cost = 85.644, x = 17.960\n",
      "EPOCH 12: Cost = 85.612, x = 17.957\n",
      "EPOCH 13: Cost = 85.580, x = 17.953\n",
      "EPOCH 14: Cost = 85.548, x = 17.950\n",
      "EPOCH 15: Cost = 85.515, x = 17.946\n",
      "EPOCH 16: Cost = 85.483, x = 17.942\n",
      "EPOCH 17: Cost = 85.451, x = 17.939\n",
      "EPOCH 18: Cost = 85.419, x = 17.935\n",
      "EPOCH 19: Cost = 85.387, x = 17.932\n",
      "EPOCH 20: Cost = 85.355, x = 17.928\n",
      "EPOCH 21: Cost = 85.322, x = 17.925\n",
      "EPOCH 22: Cost = 85.290, x = 17.921\n",
      "EPOCH 23: Cost = 85.258, x = 17.917\n",
      "EPOCH 24: Cost = 85.226, x = 17.914\n",
      "EPOCH 25: Cost = 85.194, x = 17.910\n",
      "EPOCH 26: Cost = 85.162, x = 17.907\n",
      "EPOCH 27: Cost = 85.130, x = 17.903\n",
      "EPOCH 28: Cost = 85.098, x = 17.899\n",
      "EPOCH 29: Cost = 85.066, x = 17.896\n",
      "EPOCH 30: Cost = 85.034, x = 17.892\n",
      "EPOCH 31: Cost = 85.002, x = 17.889\n",
      "EPOCH 32: Cost = 84.970, x = 17.885\n",
      "EPOCH 33: Cost = 84.938, x = 17.882\n",
      "EPOCH 34: Cost = 84.906, x = 17.878\n",
      "EPOCH 35: Cost = 84.874, x = 17.874\n",
      "EPOCH 36: Cost = 84.842, x = 17.871\n",
      "EPOCH 37: Cost = 84.810, x = 17.867\n",
      "EPOCH 38: Cost = 84.778, x = 17.864\n",
      "EPOCH 39: Cost = 84.746, x = 17.860\n",
      "EPOCH 40: Cost = 84.714, x = 17.857\n",
      "EPOCH 41: Cost = 84.682, x = 17.853\n",
      "EPOCH 42: Cost = 84.650, x = 17.849\n",
      "EPOCH 43: Cost = 84.619, x = 17.846\n",
      "EPOCH 44: Cost = 84.587, x = 17.842\n",
      "EPOCH 45: Cost = 84.555, x = 17.839\n",
      "EPOCH 46: Cost = 84.523, x = 17.835\n",
      "EPOCH 47: Cost = 84.491, x = 17.832\n",
      "EPOCH 48: Cost = 84.459, x = 17.828\n",
      "EPOCH 49: Cost = 84.428, x = 17.824\n",
      "EPOCH 50: Cost = 84.396, x = 17.821\n",
      "EPOCH 51: Cost = 84.364, x = 17.817\n",
      "EPOCH 52: Cost = 84.332, x = 17.814\n",
      "EPOCH 53: Cost = 84.301, x = 17.810\n",
      "EPOCH 54: Cost = 84.269, x = 17.807\n",
      "EPOCH 55: Cost = 84.237, x = 17.803\n",
      "EPOCH 56: Cost = 84.206, x = 17.800\n",
      "EPOCH 57: Cost = 84.174, x = 17.796\n",
      "EPOCH 58: Cost = 84.142, x = 17.792\n",
      "EPOCH 59: Cost = 84.111, x = 17.789\n",
      "EPOCH 60: Cost = 84.079, x = 17.785\n",
      "EPOCH 61: Cost = 84.047, x = 17.782\n",
      "EPOCH 62: Cost = 84.016, x = 17.778\n",
      "EPOCH 63: Cost = 83.984, x = 17.775\n",
      "EPOCH 64: Cost = 83.953, x = 17.771\n",
      "EPOCH 65: Cost = 83.921, x = 17.767\n",
      "EPOCH 66: Cost = 83.889, x = 17.764\n",
      "EPOCH 67: Cost = 83.858, x = 17.760\n",
      "EPOCH 68: Cost = 83.826, x = 17.757\n",
      "EPOCH 69: Cost = 83.795, x = 17.753\n",
      "EPOCH 70: Cost = 83.763, x = 17.750\n",
      "EPOCH 71: Cost = 83.732, x = 17.746\n",
      "EPOCH 72: Cost = 83.700, x = 17.743\n",
      "EPOCH 73: Cost = 83.669, x = 17.739\n",
      "EPOCH 74: Cost = 83.637, x = 17.736\n",
      "EPOCH 75: Cost = 83.606, x = 17.732\n",
      "EPOCH 76: Cost = 83.574, x = 17.728\n",
      "EPOCH 77: Cost = 83.543, x = 17.725\n",
      "EPOCH 78: Cost = 83.512, x = 17.721\n",
      "EPOCH 79: Cost = 83.480, x = 17.718\n",
      "EPOCH 80: Cost = 83.449, x = 17.714\n",
      "EPOCH 81: Cost = 83.417, x = 17.711\n",
      "EPOCH 82: Cost = 83.386, x = 17.707\n",
      "EPOCH 83: Cost = 83.355, x = 17.704\n",
      "EPOCH 84: Cost = 83.323, x = 17.700\n",
      "EPOCH 85: Cost = 83.292, x = 17.697\n",
      "EPOCH 86: Cost = 83.261, x = 17.693\n",
      "EPOCH 87: Cost = 83.229, x = 17.689\n",
      "EPOCH 88: Cost = 83.198, x = 17.686\n",
      "EPOCH 89: Cost = 83.167, x = 17.682\n",
      "EPOCH 90: Cost = 83.136, x = 17.679\n",
      "EPOCH 91: Cost = 83.104, x = 17.675\n",
      "EPOCH 92: Cost = 83.073, x = 17.672\n",
      "EPOCH 93: Cost = 83.042, x = 17.668\n",
      "EPOCH 94: Cost = 83.011, x = 17.665\n",
      "EPOCH 95: Cost = 82.979, x = 17.661\n",
      "EPOCH 96: Cost = 82.948, x = 17.658\n",
      "EPOCH 97: Cost = 82.917, x = 17.654\n",
      "EPOCH 98: Cost = 82.886, x = 17.651\n",
      "EPOCH 99: Cost = 82.855, x = 17.647\n",
      "EPOCH 100: Cost = 82.824, x = 17.644\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given the starting point of any `x` gradient descent\n",
    "should be able to find the minimum value of x for the\n",
    "cost function `f` defined below.\n",
    "\"\"\"\n",
    "import random\n",
    "# from gd import gradient_descent_update\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Quadratic function.\n",
    "\n",
    "    It's easy to see the minimum value of the function\n",
    "    is 5 when is x=0.\n",
    "    \"\"\"\n",
    "    return x**2 + 5\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"\n",
    "    Derivative of `f` with respect to `x`.\n",
    "    \"\"\"\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "# Random number better 0 and 10,000. Feel free to set x whatever you like.\n",
    "x = random.randint(0, 10)\n",
    "# TODO: Set the learning rate\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    cost = f(x)\n",
    "    gradx = df(x)\n",
    "    print(\"EPOCH {}: Cost = {:.3f}, x = {:.3f}\".format(i, cost, gradx))\n",
    "    x = gradient_descent_update(x, gradx, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
